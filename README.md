Hallucination Detection in QA Systems

This project focuses on detecting hallucinations in Question-Answering (QA) systems. Hallucinations refer to instances where the system generates an answer that is not supported by the provided context. This repository includes a machine learning pipeline to preprocess the data, train a logistic regression model, tune hyperparameters, and evaluate the model's performance.

Dataset

The dataset used in this project is Hallucination-Dataset-400-Samples.csv, which consists of 400 samples with the following columns:

Context: The context or passage.

Question: The question related to the context.

Answer: The answer generated by the QA system.

Hallucination: Binary label indicating if the answer is a hallucination (1) or not (0).

Requirements

To run this project, you need the following libraries:

pandas

scikit-learn

You can install the required libraries using:

pip install pandas scikit-learn

Usage:

Load and Preprocess Data:

The dataset is loaded, and any missing values are replaced with empty strings.

Split the Dataset:

The data is split into training and testing sets (60-40 split).

Define Preprocessing Steps:

Text data in the 'Context', 'Question', and 'Answer' columns is transformed using TfidfVectorizer.

Define and Train the Model:

A logistic regression model is trained using a pipeline that includes preprocessing steps. Hyperparameters are tuned using GridSearchCV.

Evaluate the Model:

The model's performance is evaluated using accuracy, AUC-ROC score, and F1 score.

Predict and Analyze Results:

Predictions are made on the testing set and the complete dataset. The results, including the counts of true labels and predictions, are printed and analyzed.
